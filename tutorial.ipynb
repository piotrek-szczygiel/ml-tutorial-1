{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gwq-mRl6hjxy"
   },
   "source": [
    "# 1. Zbiory danych\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2dnym8jZsp4s"
   },
   "source": [
    "Zainstaluj potrzebne biblioteki:\n",
    "\n",
    "`pip install numpy pandas matplotlib sklearn imgaug --user`\n",
    "\n",
    "\n",
    "Do wczytywania zbiorów danych wykorzystamy bibliotekę Tensorflow.\n",
    "\n",
    "Python 3.8: `pip install tf-nightly --user`\n",
    "\n",
    "Python <=3.7: `pip install tensorflow --user`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wMTkP_OAPp-5"
   },
   "source": [
    "## MNIST\n",
    "\n",
    "\"The MNIST database of handwritten digits, available from this page, has a training set of 60,000 examples, and a test set of 10,000 examples. It is a subset of a larger set available from NIST. The digits have been size-normalized and centered in a fixed-size image.\"\n",
    "\n",
    "http://yann.lecun.com/exdb/mnist/\n",
    "\n",
    "![MNIST](https://upload.wikimedia.org/wikipedia/commons/2/27/MnistExamples.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_XXjA09gYjWg"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "len(x_train), len(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aulGN3JRZenf"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(x_train[777], cmap='gray')\n",
    "y_train[777]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lbba2KauS9Yu"
   },
   "source": [
    "## FMNIST\n",
    "\n",
    "\"Fashion-MNIST is a dataset of Zalando's article images—consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes. We intend Fashion-MNIST to serve as a direct drop-in replacement for the original MNIST dataset for benchmarking machine learning algorithms. It shares the same image size and structure of training and testing splits.\"\n",
    "\n",
    "https://github.com/zalandoresearch/fashion-mnist\n",
    "\n",
    "![FMNIST](https://www.surajx.in/images/fashion_random.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZRP6EpftZM9A"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "len(x_train), len(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u2hE4HjpZ35G"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(x_train[42], cmap='gray')\n",
    "y_train[42]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "48g5Eh7aTTG6"
   },
   "source": [
    "## CIFAR-10\n",
    "\n",
    "\"The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images.\"\n",
    "\n",
    "https://www.cs.toronto.edu/~kriz/cifar.html\n",
    "\n",
    "![alt text](https://miro.medium.com/max/944/1*6XQqOifwnmplS22zCRRVaw.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xYHOkoYvbcw-"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "len(x_train), len(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mssJCFUvbw5x"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(x_train[1410])\n",
    "y_train[1410]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A-BxwBOXTfMd"
   },
   "source": [
    "## CIFAR-100\n",
    "\n",
    "\"This dataset is just like the CIFAR-10, except it has 100 classes containing 600 images each.\"\n",
    "\n",
    "https://www.cs.toronto.edu/~kriz/cifar.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IiAGoxUKbfb4"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar100.load_data()\n",
    "\n",
    "len(x_train), len(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z-8KFYrbTkzp"
   },
   "source": [
    "## SmallNorb\n",
    "\n",
    "\"This database is intended for experiments in 3D object reocgnition from shape. It contains images of 50 toys belonging to 5 generic categories: four-legged animals, human figures, airplanes, trucks, and cars. The objects were imaged by two cameras under 6 lighting conditions, 9 elevations (30 to 70 degrees every 5 degrees), and 18 azimuths (0 to 340 every 20 degrees).\"\n",
    "\n",
    "https://cs.nyu.edu/~ylclab/data/norb-v1.0-small/\n",
    "\n",
    "![alt text](https://www.researchgate.net/profile/Hugo_Penedones/publication/330412104/figure/fig1/AS:715640818114567@1547633475277/The-NORB-dataset-with-objects-within-a-category-aligned-at-azimuth-zero.ppm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bJR5HmRrT0yZ"
   },
   "source": [
    "## TNG\n",
    "\n",
    "\"This dataset contains all episodes of star trek TNG and has seperate rows for every speech or description that I found in the moviescripts.\"\n",
    "\n",
    "https://github.com/RMHogervorst/TNG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yPFJPb4fhtM4"
   },
   "source": [
    "# 2. Metody normalizacji danych\n",
    "\n",
    "## Selekcja \n",
    "\n",
    "Przykładowa implementacja funkcji wybierającej *n* elementów (wierszy) z podanego zbioru danych w postaci tabelarycznej (pd.DataFrame)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UkEcyNOKbKEX"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def select_n(filename, n=1000):\n",
    "    df = pd.read_csv(filename)\n",
    "    return df.sample(n=n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "35WGHWu7vjUQ"
   },
   "source": [
    "W przypadku gdy zbiorem danych jest macierz (np.array), traktujemy jej wiersze jako próbki (sample), a kolumny jako wektory cech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3j6henweviLL"
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "\n",
    "# data = [[height1, weight1],\n",
    "#         [height2, weight2],\n",
    "#         [height3, weight3]]\n",
    "\n",
    "data_train = np.array([\n",
    "  [163, 58],\n",
    "  [150, 54],\n",
    "  [175, 73],\n",
    "  [186, 95],\n",
    "  [149, 57],\n",
    "  [162, 68],\n",
    "  [177, 90],\n",
    "  [181, 86],\n",
    "  [163, 72],\n",
    "])\n",
    "data_test = np.array([\n",
    "  [150, 54],\n",
    "  [169, 64],\n",
    "  [187, 82],\n",
    "  [173, 90],\n",
    "  [162, 70],\n",
    "  [142, 60],\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "defSsON9t7ku"
   },
   "source": [
    "## Skalowanie\n",
    "\n",
    "$ X' = \\dfrac{X - \\min(X)}{\\max(X) - \\min(X)} $\n",
    "\n",
    "https://scikit-learn.org/stable/modules/preprocessing.html#scaling-features-to-a-range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MxoMRjtfv9-r"
   },
   "outputs": [],
   "source": [
    "scaler = preprocessing.MinMaxScaler().fit(data_train)\n",
    "data_train_scaled = scaler.transform(data_train)\n",
    "data_train_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9BAeev9dwChn"
   },
   "outputs": [],
   "source": [
    "heights = data_train_scaled[:,0]\n",
    "min(heights), max(heights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MDagVKL2FS75"
   },
   "source": [
    "## Standaryzacja\n",
    "\n",
    "Funkcja standaryzująca wartości wektorów cech\n",
    "\n",
    "$ X' = \\dfrac{X-\\mu_X}{\\sigma_X} $\n",
    "\n",
    "wartość oczekiwana: $ \\mu_{X'} = 0 $\n",
    "\n",
    "odchylenie standardowe: $ \\sigma_{X'} = 1 $\n",
    "\n",
    "https://scikit-learn.org/stable/modules/preprocessing.html#preprocessing-scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mqfVkoxdFgi_"
   },
   "outputs": [],
   "source": [
    "scaler = preprocessing.StandardScaler().fit(data_train)\n",
    "data_train_scaled = scaler.transform(data_train)\n",
    "data_train_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p6c3m7VmHcf_"
   },
   "outputs": [],
   "source": [
    "data_train_scaled.mean(axis=0)  # should be [0., 0.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BRPDB9CoHoxj"
   },
   "outputs": [],
   "source": [
    "data_train_scaled.std(axis=0)  # should be [1., 1.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cfoDSAnuIkgU"
   },
   "outputs": [],
   "source": [
    "data_test_scaled = scaler.transform(data_test)\n",
    "data_test_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ml7UrrhdInE-"
   },
   "outputs": [],
   "source": [
    "data_test_scaled.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mWgWPs5ZIn-z"
   },
   "outputs": [],
   "source": [
    "data_test_scaled.std(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XaZDOGxVui0E"
   },
   "source": [
    "## Normalizacja\n",
    "\n",
    "\"Normalization is the process of scaling individual samples to have unit norm (vector length equal to 1).\"\n",
    "\n",
    "https://scikit-learn.org/stable/modules/preprocessing.html#preprocessing-normalization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PA3GmVDmwJpp"
   },
   "outputs": [],
   "source": [
    "scaler = preprocessing.Normalizer().fit(data_train)\n",
    "data_train_scaled = scaler.transform(data_train)\n",
    "data_train_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q7evsxQ4w0Yi"
   },
   "outputs": [],
   "source": [
    "np.linalg.norm(data_train_scaled, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SUVjYjhMuC8O"
   },
   "source": [
    "## UCI Machine Learning Repository\n",
    "\n",
    "https://archive.ics.uci.edu/ml/datasets.php?format=&task=cla&att=num&area=&numAtt=10to100&numIns=greater1000&type=&sort=attDown&view=table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9vN9dqvHKd7D"
   },
   "source": [
    "Wczytujemy zbiór danych [Character Font Images Data Set](https://archive.ics.uci.edu/ml/datasets/Character+Font+Images?fbclid=IwAR2abp7ksO_B30IQP6vozDmbTyp4X0UcvZF8lV7rMWeKWtvqu3OqwdjAudY/) ze strony UCI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m4gj5_u76FUU"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def to_img(df):\n",
    "  assert pd.unique(df['h']).size == 1\n",
    "  assert pd.unique(df['w']).size == 1\n",
    "\n",
    "  h, w = df['h'].iloc[0], df['w'].iloc[0]\n",
    "  images = np.empty((len(df), w, h))\n",
    "\n",
    "  for x in range(w):\n",
    "    for y in range(h):\n",
    "      label = 'r{}c{}'.format(y, x)\n",
    "      images[:, y, x] = df[label].values\n",
    "  return images\n",
    "\n",
    "def plot_fonts(images, n_cols=10):\n",
    "  n_rows = len(images) // n_cols + 1\n",
    "\n",
    "  fig, axes = plt.subplots(nrows=n_rows, ncols=n_cols, squeeze=False)\n",
    "  fig.set_size_inches(20, 2 * n_rows)\n",
    "  \n",
    "  for i, img in enumerate(images):\n",
    "    ax = axes[i // n_cols][i % n_cols]\n",
    "    ax.imshow(img, cmap='gray', vmin=0, vmax=255)\n",
    "    ax.set_xticks([]); ax.set_yticks([])\n",
    "\n",
    "  for i in range(len(images), n_rows * n_cols):\n",
    "    axes[i // n_cols][i % n_cols].axis('off')\n",
    "\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t3DQfWzd-zvY"
   },
   "outputs": [],
   "source": [
    "times_df = select_n('./fonts/TIMES.csv')\n",
    "roman_df = select_n('./fonts/ROMAN.csv')\n",
    "times_img = to_img(times_df)\n",
    "roman_img = to_img(roman_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Rt1vZNMWgfgU"
   },
   "source": [
    "# 3. Algorytm *k-nearest neighbors* (k-NN).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dPzUI89jJLf8"
   },
   "source": [
    "Implementacje metod opartych na wyszukiwaniu najbliższych sąsiadów można znaleźć w pakiecie **sklearn.neighbors**. Przedstawimy ich działanie na modelu, który rozróżnia bitmapy znaków czcionki Times vs. Roman.\n",
    "\n",
    "Najpierw zobaczmy przykładowe znaki obu klas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GCZLhNaj84nA"
   },
   "outputs": [],
   "source": [
    "plot_fonts(times_img[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uWwVThJ1BQ9m"
   },
   "outputs": [],
   "source": [
    "plot_fonts(roman_img[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MjC2OKeaNhAL"
   },
   "source": [
    "### **Wyznaczanie najbliższych sąsiadów.**\n",
    "Można użyć klasy *NearestNeighbors*. Opiszemy ważniejsze parametry (poza liczbą sąsiadów *n_neighbors*):\n",
    "\n",
    "**-> algorithm**: mamy do wyboru różne metody znajdowania sąsiadów. Dla zbioru treningowego M punktów (x<sub>1</sub>, x<sub>2</sub>, ..., x<sub>d</sub>):\n",
    "\n",
    "  - metoda *brute* wyznacza dla każdego punktu jego odległość od wszystkich pozostałych. \n",
    "    - złożoność czasowa: __O(d * M^2)__\n",
    " \n",
    "  - metoda *kd_tree* tworzy drzewo binarne, rekurencyjnie dzieląc zbiór treningowy na podzbiory nierównościami typu x<sub>i</sub> <= a<sub>j</sub>, gdzie a<sub>j</sub> jest ustalane w oparciu o dane. \n",
    "    - złożoność czasowa: dla małych wymiarów D (np. < 20) w przybliżeniu __O(d \\* M \\* log(M))__, dla większych zbliża się do  __O(d \\* M^2)__\n",
    "\n",
    "- metoda *ball_tree* również tworzy drzewo binarne, ale podzbiór odpowiadający danemu węzłowi nie jest dzielony na dwoje w oparciu o wartość pojedynczej cechy jak dla *kd_tree*. Zamiast tego do podzbioru dopasowywane są 2 d-wymiarowe kule, do których przyporządkowuje się punkty tego podzbioru - w wyniku tego powstają 2 podzbiory (niższe węzły drzewa)\n",
    "  - złożoność czasowa: średnio __O(d \\* M \\* log(M))__\n",
    "\n",
    "- metoda *auto* wybiera najlepszy algorytm z powyższych zależnie od danych w zbiorze treningowym\n",
    "- więcej szczegółów i wskazówki dot. wyboru parametrów można znaleźć [ tutaj ](https://scikit-learn.org/stable/modules/neighbors.html#nearest-neighbor-algorithms)\n",
    "\n",
    "**-> metric**: standardowo odległość Euklidesowa (metryka Minkowskiego z p=2), można przekazać dowolną funkcję z **sklearn.neighbors.DistanceMetric**\n",
    "\n",
    "**-> p**: wartość p dla metryki Minkowskiego\n",
    "\n",
    "**-> n_jobs**: liczba używanych procesów"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c0FklRUdtWYW"
   },
   "source": [
    "Wyznaczymy teraz najbliższych sąsiadów w załadowanym zbiorze. Tworzymy w tym celu 2-wymiarową tablicę X i wektor odpowiedzi y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9Y8FsVr2gYjg"
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "assert times_img.shape[1:] == roman_img.shape[1:]\n",
    "\n",
    "w, h = times_img.shape[1:]\n",
    "X = np.concatenate([times_img, roman_img])\n",
    "X = X.reshape((X.shape[0], -1))\n",
    "y = np.concatenate([np.zeros(len(times_img)), np.ones(len(roman_img))])\n",
    "\n",
    "invalid_samples = np.any(np.isnan(X), axis=1)\n",
    "X = X[~invalid_samples]\n",
    "y = y[~invalid_samples]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5lbiiADfpci_"
   },
   "source": [
    "Dopasowujemy *Ball Tree* do danych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JPB3cBGG0DTC"
   },
   "outputs": [],
   "source": [
    "k = 50\n",
    "nbrs = NearestNeighbors(n_neighbors=k, algorithm='ball_tree').fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oq3tj2e4flzU"
   },
   "source": [
    "Wybierzmy przykładowy obrazek i sprawdźmy, jak wyglądają jego najbliżsi sąsiedzi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yscWCDsFfeqX"
   },
   "outputs": [],
   "source": [
    "target_ind = len(times_img) + 42\n",
    "target_img = X[target_ind].reshape((1, w, h))\n",
    "plot_fonts(target_img, n_cols=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n4Uez0hFeVix"
   },
   "outputs": [],
   "source": [
    "distances, indices = nbrs.kneighbors(target_img.reshape(1, -1))\n",
    "neighbor_inds = indices[0][1:]  # indices[0][0] is target_img index\n",
    "\n",
    "neighbors = X[neighbor_inds].reshape((k - 1, w, h))\n",
    "plot_fonts(neighbors, n_cols=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "reR-gn2Yk6M0"
   },
   "source": [
    "Jak widać, najbliżsi sąsiedzi faktycznie odpowiadają wybranemu obrazkowi, dalsi niekoniecznie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C-98lq0CQ_lQ"
   },
   "source": [
    "### **Klasyfikacja.**\n",
    "Można użyć klasy *KNeighborsClassifier*. Do dyspozycji mamy te parametry co poprzednio, a także parametr *weights*. Domyślnie wagi są sobie równe, podanie wartości *distance* zmniejsza je wraz z ze wzrostem odległości."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yDNqa0jJqzcW"
   },
   "source": [
    "Sprawdzimy, jak nasz klasyfikator poradzi sobie z odróżnianiem znaków czcionki Arial od znaków czcionki Consolas.\n",
    "W tym celu dzielimy zbiór X na zbiory treningowy i testowy. Najpierw jednak zmniejszamy rozmiar zbioru, aby skrócić czas działania."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B9Ggx-PMQ8ca"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "\n",
    "sub_size = int(len(X) * 0.1)\n",
    "X_sub, y_sub = resample(X, y, n_samples=sub_size, replace=False, random_state=0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_sub, y_sub, test_size=0.4, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c646e3NFsF8M"
   },
   "source": [
    "Następnie dopasowujemy klasyfikator do zbioru treningowego i sprawdzamy odsetek poprawnych predykcji na zbiorze testowym."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Lc3KRJQHr_AP"
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "neigh = KNeighborsClassifier(n_neighbors=3, algorithm='auto')\n",
    "neigh.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6CoZ91W-s8U2"
   },
   "outputs": [],
   "source": [
    "y_pred = neigh.predict(X_test)\n",
    "accuracy_score(y_true=y_test, y_pred=y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-UjA1jBRvjex"
   },
   "source": [
    "Wynik jest zaskakująco wysoki. W następnym kroku chcielibyśmy sprawdzić, czy da się go poprawić, modyfikując hiperparametry modelu (przede wszystkim liczbę sąsiadów k)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jqyaY_YYhY3_"
   },
   "source": [
    "# 4. *Cross-validation*.\n",
    "\n",
    "Jeśli jako ostateczne wybierzemy hiperparametry, dla których model uzyskuje najlepszą wartość metryki na zbiorze testowym, to wynik ten prawdopodobnie będzie lepszy niż wynik dla danych, z którymi model nie miał styczności. Inaczej mówiąc, model ten jest \"dopasowany\" do konkretnego zbioru testowego.\n",
    "\n",
    "Z tego powodu dane można podzielić na zbiór treningowy, walidacyjny i testowy, np. w proporcji 60:20:20, i ostateczne hiperparametry wybrać w oparciu o wynik na zbiorze walidacyjnym. Wynik na zbiorze testowym daje zaś wiarygodne oszacowanie jakości modelu dla nowych danych.\n",
    "\n",
    "Powoduje to jednak znaczne zmniejszenie zbioru treningowego, co może zmniejszyć wiarygodność rezultatu. Przeciwdziałamy temu, stosując cross-validation. \n",
    "\n",
    "Dla przykładu, *k*-krotna cross-walidacja polega na podzieleniu danych na zbiór cross-walidacyjny i testowy, np. w proporcji 80:20. Następnie, zbiór cross-walidacyjny dzieli się na k części. Jedną z nich traktuje się jako zbiór walidacyjny, a pozostałe łącznie - jako treningowy. Na tych danych trenuje się i waliduje model z ustalonymi hiperparametrami. Ta procedura jest wykonywana k-krotnie, za każdym razem wybieramy inną część jako zbiór walidacyjny. \n",
    "\n",
    "Uzyskuje się w ten sposób k wartości metryki, które można uśrednić, uzyskując bardziej wiarygodny wynik dla danych hiperparametrów. Jako ostateczne wybiera się hiperparametry, dla których uzyskana średnia była najlepsza. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ICwAo5p5RfH5"
   },
   "source": [
    "Dla przykładu, wyznaczamy wartości *accuracy* dopasowanego modelu w 5-krotnej cross-walidacji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aFk5uODFRdqc"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(neigh, X_train, y_train, cv=5)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WO5NG61VrC62"
   },
   "source": [
    "# 5. Wpływ hiperparametrów i rozmiaru zbioru treningowego na jakość klasyfikatora."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wawCn6gbMcCF"
   },
   "source": [
    "Zbadamy teraz wpływ hiperparametrów takich jak liczba sąsiadów k i użyte wagi na wynik cross-validacji dla modelu k-NN. Sprawdzimy również, jakie znaczenie ma rozmiar zbioru treningowego w stosunku do rozmiaru zbioru testowego.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GZt98-7xscAy"
   },
   "outputs": [],
   "source": [
    "def check(X, y, fraction=0.1):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=0)\n",
    "    sub_size = int(len(X_train) * fraction)\n",
    "    X_train, y_train = resample(X_train, y_train, n_samples=sub_size, replace=False, random_state=0)\n",
    "\n",
    "    model = KNeighborsClassifier(n_neighbors=5, algorithm='auto')\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    return model, accuracy_score(y_true=y_test, y_pred=y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "whVbuV6Yxoqd"
   },
   "outputs": [],
   "source": [
    "check(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wWvrlKkLyNGA"
   },
   "outputs": [],
   "source": [
    "check(X, y, fraction=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QlUg-nq_sFCu"
   },
   "source": [
    "Bardzo użyteczna jest klasa *GridSearchCV*, której można użyć do przeprowadzenia cross-walidacji dla wielu różnych kombinacji hiperparametrów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B93iSEmlLiRI"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = {\n",
    "  'n_neighbors': (2, 3, 5),\n",
    "  'weights': ('uniform', 'distance')\n",
    "}\n",
    "knn = KNeighborsClassifier(algorithm='auto')\n",
    "clf = GridSearchCV(knn, params)\n",
    "\n",
    "sub_size = int(len(X) * 0.1)\n",
    "X_sub, y_sub = resample(X, y, n_samples=sub_size, replace=False, random_state=0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_sub, y_sub, test_size=0.4, random_state=0)\n",
    "\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "82R3G1mlVjVo"
   },
   "source": [
    "Po wykonaniu metody *fit* mamy dostęp do wyników cross-walidacji dla poszczególnych zestawów hiperparametrów, a także do najlepszego spośród nich.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vEU7dv9ZUHQw"
   },
   "outputs": [],
   "source": [
    "def grid_cv_table(cv_results):\n",
    "    param_cols = pd.DataFrame(clf.cv_results_['params'])\n",
    "    score_cols = pd.DataFrame(clf.cv_results_['mean_test_score'], columns=['eval_score'])\n",
    "    est_df = pd.concat([param_cols, score_cols], axis=1)\n",
    "    return est_df.set_index(clf.cv_results_['rank_test_score']).sort_index()\n",
    "\n",
    "grid_cv_table(clf.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ElMaW54TWBeI"
   },
   "outputs": [],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PnX7D-RRVHpW"
   },
   "source": [
    "Można użyć najlepszych znalezionych hiperparametrów do określenia *accuracy* na zbiorze testowym."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bqudt6QpUbjU"
   },
   "outputs": [],
   "source": [
    "y_pred_best = clf.predict(X_test)\n",
    "accuracy_score(y_true=y_test, y_pred=y_pred_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lYES3i1M1gB3"
   },
   "source": [
    "### Inne miary jakości w klasyfikacji\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qo5avy0Y1l1K"
   },
   "source": [
    "Klasycznym przykładem niedoskonałości *accuracy* jako miary jakości klasyfikatora jest problem niezrównoważonych klas. Jeśli 99% zbioru testowego stanowią przykłady klasy 0, a tylko 1% klasy 1, to klasyfikator zwracający zawsze 0 osiąga dokładność 99%.\n",
    "\n",
    "Wprowadza się więc miary *precision* i *recall*, definiowane następująco:\n",
    "\n",
    "$precision = \\frac{TP}{TP + FP}$\n",
    "\n",
    "$recall = \\frac{TP}{TP + FN},$\n",
    "\n",
    "gdzie:\n",
    " - TP to liczba *true positives*, czyli przykładów klasy 1, które zostały poprawnie zaklasyfikowane jako 1,\n",
    " - FP to liczba *false positives* (zaklasyfikowane jako 1, naprawdę klasy 0),\n",
    " - FN to liczba *false negatives* (zaklasyfikowane jako 0, naprawdę klasy 1).\n",
    "\n",
    "Wysokie *precision* oznacza, że duży odsetek pozytywnych predykcji naszego klasyfikatora jest poprawny (w powyższym przykładzie jest niezdefiniowane - brak pozytywnych predykcji). Wysokie *recall*, że niewiele przykładów, które faktycznie są pozytywne mu \"umyka\" (daje predykcję 0) (w powyższym przykładzie 0%).\n",
    "\n",
    "Aby zbalansować obie te wielkości, stosuje się F1 score, zdefiniowany jako\n",
    "\n",
    "$F1 = \\frac{2}{\\frac{1}{precision} + \\frac{1}{recall}}$\n",
    "\n",
    "Żeby przyjmował duże wartości, zarówno *precision* jak i *recall* muszą być wysokie.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PqNquk3U1pCE"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1_score(y_true=y_test, y_pred=y_pred_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4_oxCvvC1vp2"
   },
   "source": [
    "Sprawdźmy jeszcze, czy model najlepszy pod względem *accuracy* jest też najlepszy pod względem F1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xsRNA76E1tZe"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "f1_scorer = make_scorer(f1_score)\n",
    "clf = GridSearchCV(knn, params, scoring=f1_scorer)\n",
    "clf.fit(X_train, y_train)\n",
    "grid_cv_table(clf.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Augumentacja danych.\n",
    "\n",
    "Stwórzmy prostą funkcję, która będzie dodawała szum do przekazanego jej obrazka."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def salt_and_pepper(image, percent=0.05):\n",
    "    h, w = image.shape\n",
    "    pixels = np.random.randint(0, w * h, int(percent * w * h))\n",
    "    for p in pixels:\n",
    "        image[p // w][p % w] = np.random.randint(0, 256)\n",
    "    return image\n",
    "\n",
    "plot_fonts([times_img[0]])\n",
    "plot_fonts([salt_and_pepper(np.copy(times_img[0]), percent=0.1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "X = np.concatenate([times_img, roman_img])\n",
    "y = np.concatenate([np.zeros(len(times_img)), np.ones(len(roman_img))])\n",
    "\n",
    "sub_size = int(len(X) * 0.5)\n",
    "X_sub, y_sub = resample(X, y, n_samples=sub_size, replace=False, random_state=0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_sub, y_sub, test_size=0.4, random_state=0)\n",
    "\n",
    "X_train_a = np.copy(X_train)\n",
    "for train in X_train_a:\n",
    "    salt_and_pepper(train, percent=0.05)\n",
    "\n",
    "y_train_a = y_train\n",
    "\n",
    "X_train_aug = np.concatenate([X_train, X_train_a])\n",
    "y_train_aug = np.concatenate([y_train, y_train_a])\n",
    "\n",
    "X_train_aug, y_train_aug = shuffle(X_train_aug, y_train_aug)\n",
    "\n",
    "X_test = X_test.reshape((X_test.shape[0], -1))\n",
    "X_train_aug = X_train_aug.reshape((X_train_aug.shape[0], -1))\n",
    "\n",
    "params = {\n",
    "  'n_neighbors': (2, 3, 5),\n",
    "  'weights': ('uniform', 'distance')\n",
    "}\n",
    "knn = KNeighborsClassifier(algorithm='auto')\n",
    "clf = GridSearchCV(knn, params)\n",
    "clf.fit(X_train_aug, y_train_aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_cv_table(clf.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_best = clf.predict(X_test)\n",
    "accuracy_score(y_true=y_test, y_pred=y_pred_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y_true=y_test, y_pred=y_pred_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NImUvY-8FR4g"
   },
   "source": [
    "# 7. Procedury augumentacyjne z biblioteki imgaug.\n",
    "\n",
    "\"This python library helps you with augmenting images for your machine learning projects. It converts a set of input images into a new, much larger set of slightly altered images.\"\n",
    "\n",
    "https://github.com/aleju/imgaug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GX-XYdrvFcQW"
   },
   "outputs": [],
   "source": [
    "import imgaug as ia\n",
    "import imgaug.augmenters as iaa\n",
    "\n",
    "N = 100\n",
    "\n",
    "def augment(images):\n",
    "    seq = iaa.Sequential(\n",
    "        [\n",
    "            iaa.Crop(px=(1, 2), keep_size=True),\n",
    "            iaa.SomeOf((0, 3),\n",
    "                [\n",
    "                    iaa.Affine(\n",
    "                        scale={\"x\": (0.8, 1.2), \"y\": (0.8, 1.2)},\n",
    "                        rotate=(-20, 20),\n",
    "                        order=[0, 1],\n",
    "                        cval=(0, 255),\n",
    "                        mode=ia.ALL,\n",
    "                    ),\n",
    "                    iaa.GaussianBlur((0, 0.5)),\n",
    "                    iaa.SaltAndPepper(0.05),\n",
    "                ],\n",
    "                random_order=True\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    imagesX = images[:, :, :, np.newaxis]\n",
    "    images_aug = seq(images=imagesX)\n",
    "    return [np.squeeze(img) for img in images_aug]\n",
    "\n",
    "times_aug = augment(times_img[:N])\n",
    "plot_fonts(times_aug)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZKZOq0Q32XEc"
   },
   "source": [
    "Sprawdźmy, czy zdołamy uzyskać lepszy wynik klasyfikacji, jeśli wytrenujemy model po augmentacji obu klas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1Orvk3Ki2gJ4"
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "X = np.concatenate([times_img, roman_img])\n",
    "y = np.concatenate([np.zeros(len(times_img)), np.ones(len(roman_img))])\n",
    "\n",
    "sub_size = int(len(X) * 0.5)\n",
    "X_sub, y_sub = resample(X, y, n_samples=sub_size, replace=False, random_state=0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_sub, y_sub, test_size=0.4, random_state=0)\n",
    "\n",
    "X_train_a = augment(X_train)\n",
    "y_train_a = y_train\n",
    "\n",
    "X_train_aug = np.concatenate([X_train, X_train_a])\n",
    "y_train_aug = np.concatenate([y_train, y_train_a])\n",
    "\n",
    "X_train_aug, y_train_aug = shuffle(X_train_aug, y_train_aug)\n",
    "\n",
    "X_test = X_test.reshape((X_test.shape[0], -1))\n",
    "X_train_aug = X_train_aug.reshape((X_train_aug.shape[0], -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0EmCHDMzaPOB"
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "  'n_neighbors': (2, 3, 5),\n",
    "  'weights': ('uniform', 'distance')\n",
    "}\n",
    "knn = KNeighborsClassifier(algorithm='auto')\n",
    "clf = GridSearchCV(knn, params)\n",
    "clf.fit(X_train_aug, y_train_aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OXfsKzXnaTP5"
   },
   "outputs": [],
   "source": [
    "grid_cv_table(clf.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lzBzBlToaVzp"
   },
   "outputs": [],
   "source": [
    "y_pred_best = clf.predict(X_test)\n",
    "accuracy_score(y_true=y_test, y_pred=y_pred_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iyc1L-uQaZfl"
   },
   "outputs": [],
   "source": [
    "f1_score(y_true=y_test, y_pred=y_pred_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gP25SNbUcyct"
   },
   "source": [
    "# 8. Projekt\n",
    "Wykonać punkty 6 i 7 na zbiorach MNIST, FMNIST przy pomocy imgaug oraz tworząc obrazki dodatkowe poprzez zaburzenie danych (np. dla x% pikseli losować liczbę 0,1 (MNIST), i odpowiednią – stopień szarości - dla zbioru FMNIST).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0h7oJT1v1wA2"
   },
   "source": [
    "a) Wczytaj zbiór danych MNIST (np. korzystając z biblioteki Tensorflow)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bBYA6HCV10AB"
   },
   "outputs": [],
   "source": [
    "# (x_train, y_train), (x_test, y_test) = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pGj7CU8h13_o"
   },
   "source": [
    "b) Wybierz 0.03 danych treningowych oraz 0.01 danych testujących."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_sfyawbW2A7C"
   },
   "outputs": [],
   "source": [
    "# something here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dhakLh4K17TU"
   },
   "source": [
    "c) Stwórz klasyfikator k-NN dla zbioru danych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EGcmfyx42CTP"
   },
   "outputs": [],
   "source": [
    "# something here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8C67CaJH18aX"
   },
   "source": [
    "d) Oblicz accuracy oraz F1 score dla klasyfikatora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0UJVC8qa2DDO"
   },
   "outputs": [],
   "source": [
    "# something here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "arHI6mSO19ts"
   },
   "source": [
    "e) Dokonaj augumentacji zbioru danych korzystając z różnych funkcji biblioteki imgaug (crop, rotation, gaussian blur, salt and pepper). Porównaj miary jakości."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pDvJBK_z2Dp5"
   },
   "outputs": [],
   "source": [
    "# something here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qcsv88DZ1-s1"
   },
   "source": [
    "\n",
    "\n",
    "f) Wykonaj kroki a-e dla zbioru danych FMNIST (fashion_mnist)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IWnLdt8S2FxK"
   },
   "outputs": [],
   "source": [
    "# something here"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "tuto1.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
